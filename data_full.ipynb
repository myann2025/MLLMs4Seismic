{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./DataPrep/images/img1_seismic.jpg\n",
      "./DataPrep/images/img2_seismic.jpg\n",
      "./DataPrep/images/img3_seismic.jpg\n",
      "./DataPrep/images/img4_seismic.jpg\n",
      "./DataPrep/images/img5_seismic.jpg\n",
      "./DataPrep/images/img6_seismic.jpg\n",
      "./DataPrep/images/img7_seismic.jpg\n",
      "./DataPrep/images/img8_seismic.jpg\n",
      "./DataPrep/images/img9_seismic.jpg\n",
      "./DataPrep/images/img10_seismic.jpg\n",
      "./DataPrep/images/img11_seismic.jpg\n",
      "./DataPrep/images/img12_seismic.jpg\n",
      "./DataPrep/images/img13_seismic.jpg\n",
      "./DataPrep/images/img14_seismic.jpg\n",
      "./DataPrep/images/img15_seismic.jpg\n",
      "./DataPrep/images/img16_seismic.jpg\n",
      "./DataPrep/images/img17_seismic.jpg\n",
      "./DataPrep/images/img18_seismic.jpg\n",
      "./DataPrep/images/img19_seismic.jpg\n",
      "./DataPrep/images/img20_seismic.jpg\n",
      "./DataPrep/images/img21_seismic.jpg\n",
      "./DataPrep/images/img22_seismic.jpg\n",
      "./DataPrep/images/img23_seismic.jpg\n",
      "./DataPrep/images/img24_seismic.jpg\n",
      "./DataPrep/images/img25_seismic.jpg\n",
      "./DataPrep/images/img26_seismic.jpg\n",
      "./DataPrep/images/img27_seismic.jpg\n",
      "./DataPrep/images/img28_seismic.jpg\n",
      "./DataPrep/images/img29_seismic.jpg\n",
      "./DataPrep/images/img30_seismic.jpg\n",
      "./DataPrep/images/img31_seismic.jpg\n",
      "./DataPrep/images/img32_seismic.jpg\n",
      "./DataPrep/images/img33_seismic.jpg\n",
      "./DataPrep/images/img34_seismic.jpg\n",
      "./DataPrep/images/img35_seismic.jpg\n",
      "./DataPrep/images/img36_seismic.jpg\n",
      "./DataPrep/images/img37_seismic.jpg\n",
      "./DataPrep/images/img38_seismic.jpg\n",
      "./DataPrep/images/img39_seismic.jpg\n",
      "./DataPrep/images/img39_seismic.jpg\n",
      "./DataPrep/images/img40_seismic.jpg\n",
      "./DataPrep/images/img41_seismic.jpg\n",
      "./DataPrep/images/img42_seismic.jpg\n",
      "./DataPrep/images/img43_seismic.jpg\n",
      "./DataPrep/images/img44_seismic.jpg\n",
      "./DataPrep/images/img45_seismic.jpg\n",
      "./DataPrep/images/img46_seismic.jpg\n",
      "./DataPrep/images/img47_seismic.jpg\n",
      "./DataPrep/images/img48_seismic.jpg\n",
      "./DataPrep/images/img49_seismic.jpg\n",
      "./DataPrep/images/img50_seismic.jpg\n",
      "50\n",
      "Dataset({\n",
      "    features: ['image_path', 'caption'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, Features, Value, Sequence,DatasetDict,load_dataset\n",
    "import json\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# definition of features\n",
    "features = Features({\n",
    "    \"image_path\": Value(\"string\"),\n",
    "    \"caption\": Value(\"string\"),\n",
    "})\n",
    "\n",
    "# load json file to create dataset\n",
    "def load_custom_dataset(json_file_path, image_folder_path):\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    dataset_data = {\n",
    "        \"image_path\": [],\n",
    "        \"caption\": [],\n",
    "    }\n",
    "    for annotation in data[\"annotations\"]:\n",
    "        image_path = os.path.join(image_folder_path, annotation[\"filepath\"])\n",
    "        print(image_path)\n",
    "        if image_path.endswith('.jpg') and image_path not in dataset_data[\"image_path\"]:\n",
    "            dataset_data[\"image_path\"].append(image_path)\n",
    "            dataset_data[\"caption\"].append(annotation[\"caption\"])\n",
    "        \n",
    "    print(len(dataset_data[\"image_path\"]))\n",
    "    \n",
    "    # create dataset\n",
    "    dataset = Dataset.from_dict(dataset_data, features=features)\n",
    "    return dataset\n",
    "\n",
    "# call\n",
    "json_file_path = \"./DataPrep/data.json\"  # JSON filepath\n",
    "image_folder_path = \"./DataPrep/images/\"  # image filepath\n",
    "dataset = load_custom_dataset(json_file_path, image_folder_path)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 48\n",
      "Validation dataset size: 2\n",
      "Test dataset size: 1\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image_path', 'caption'],\n",
      "        num_rows: 48\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image_path', 'caption'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image_path', 'caption'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#  split dataset and use 80% as training dataset\n",
    "dataset_split = dataset.train_test_split(test_size=0.05)\n",
    "\n",
    "# further split for validation and testing as 10% and 10%\n",
    "test_valid_split = dataset_split['test'].train_test_split(test_size=0.05)\n",
    "\n",
    "# finalized the training, validate and test dataset\n",
    "train_dataset = dataset_split['train']\n",
    "valid_dataset = test_valid_split['train']\n",
    "test_dataset = test_valid_split['test']\n",
    "\n",
    "# create a DatasetDict object\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': valid_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "\n",
    "# QC the size of each dataset\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(valid_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable to save train, validation and test dataset\n",
    "train_path = \"./DataPrep/data/train.json\"\n",
    "validation_path = \"./DataPrep/data/validation.json\"\n",
    "test_path = \"./DataPrep/data/test.json\"\n",
    "\n",
    "# save train, valid and test dataset from dataset_dict \n",
    "def save_dataset_to_json(dataset_dict, train_path, validation_path, test_path):\n",
    "    # save train dataset\n",
    "    with open(train_path, 'w', encoding='utf-8') as f:\n",
    "        for item in dataset_dict['train']:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    # save validation dataset\n",
    "    with open(validation_path, 'w', encoding='utf-8') as f:\n",
    "        for item in dataset_dict['validation']:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    # save testing dataset\n",
    "    with open(test_path, 'w', encoding='utf-8') as f:\n",
    "        for item in dataset_dict['test']:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "# writeout\n",
    "save_dataset_to_json(dataset_dict, train_path, validation_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "# load the dataset\n",
    "dataset_path = \"./DataPrep/data\"\n",
    "train_dataset = load_dataset('json', data_files={'train': f'{dataset_path}/train.json'})['train']\n",
    "validation_dataset = load_dataset('json', data_files={'validation': f'{dataset_path}/validation.json'})['validation']\n",
    "test_dataset = load_dataset('json', data_files={'test': f'{dataset_path}/test.json'})['test']\n",
    "\n",
    "# define a mapping function to load image to example['image']\n",
    "def preprocess_example(example):\n",
    "    example['image'] = load_image(example['image_path'])\n",
    "    return example\n",
    "\n",
    "# apply mapping function to three datasets\n",
    "train_dataset = train_dataset.map(preprocess_example)\n",
    "validation_dataset = validation_dataset.map(preprocess_example)\n",
    "test_dataset = test_dataset.map(preprocess_example)\n",
    "\n",
    "# create DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "#dump to local disk\n",
    "dataset_dict.save_to_disk('./DataPrep/data/dataset_full/')\n",
    "\n",
    "# qc the 1st element\n",
    "print(dataset_dict['train'][0])\n",
    "\n",
    "#load from disk\n",
    "from datasets import load_from_disk\n",
    "train_dataset_new = load_from_disk('./DataPrep/data/dataset_full/train')\n",
    "validation_dataset_new = load_from_disk('./DataPrep/data/dataset_full/validation')\n",
    "test_dataset_new = load_from_disk('./DataPrep/data/dataset_full/test')\n",
    "\n",
    "print(train_dataset_new[0])\n",
    "print(validation_dataset_new[0])\n",
    "print(test_dataset_new[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
